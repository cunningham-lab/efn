%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for John Cunningham at 2018-05-14 13:42:50 -0400 


%% Saved with string encoding Unicode (UTF-8) 



@article{tabak2010density,
	Author = {Tabak, Esteban G and Vanden-Eijnden, Eric and others},
	Date-Added = {2018-05-14 17:41:23 +0000},
	Date-Modified = {2018-05-14 17:41:23 +0000},
	Journal = {Communications in Mathematical Sciences},
	Number = {1},
	Pages = {217--233},
	Publisher = {International Press of Boston},
	Title = {Density estimation by dual ascent of the log-likelihood},
	Volume = {8},
	Year = {2010}}

@inproceedings{chen2001gaussianization,
	Author = {Chen, Scott Saobing and Gopinath, Ramesh A},
	Booktitle = {Advances in neural information processing systems},
	Date-Added = {2018-05-14 17:39:37 +0000},
	Date-Modified = {2018-05-14 17:39:37 +0000},
	Pages = {423--429},
	Title = {Gaussianization},
	Year = {2001}}

@article{dinh2016density,
	Author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
	Date-Added = {2018-05-14 16:41:40 +0000},
	Date-Modified = {2018-05-14 16:41:40 +0000},
	Journal = {arXiv preprint arXiv:1605.08803},
	Title = {Density estimation using Real NVP},
	Year = {2016}}

@inproceedings{cisse2017parseval,
	Author = {Cisse, Moustapha and Bojanowski, Piotr and Grave, Edouard and Dauphin, Yann and Usunier, Nicolas},
	Booktitle = {International Conference on Machine Learning},
	Date-Added = {2018-05-14 16:36:20 +0000},
	Date-Modified = {2018-05-14 16:36:20 +0000},
	Pages = {854--863},
	Title = {Parseval networks: Improving robustness to adversarial examples},
	Year = {2017}}

@article{jacobsen2018revnet,
	Author = {Jacobsen, J{\"o}rn-Henrik and Smeulders, Arnold and Oyallon, Edouard},
	Date-Added = {2018-05-14 16:28:23 +0000},
	Date-Modified = {2018-05-14 16:28:23 +0000},
	Journal = {arXiv preprint arXiv:1802.07088},
	Title = {i-RevNet: Deep Invertible Networks},
	Year = {2018}}

@inproceedings{baird2005one,
	Author = {Baird, Leemon and Smalenberger, David and Ingkiriwang, Shawn},
	Booktitle = {Neural Networks, 2005. IJCNN'05. Proceedings. 2005 IEEE International Joint Conference on},
	Date-Added = {2018-05-14 16:16:55 +0000},
	Date-Modified = {2018-05-14 16:16:55 +0000},
	Organization = {IEEE},
	Pages = {966--971},
	Title = {One-step neural network inversion with PDF learning and emulation},
	Volume = {2},
	Year = {2005}}

@article{rippel2013high,
	Author = {Rippel, Oren and Adams, Ryan Prescott},
	Date-Added = {2018-05-14 16:06:25 +0000},
	Date-Modified = {2018-05-14 16:06:25 +0000},
	Journal = {arXiv preprint arXiv:1302.5125},
	Title = {High-dimensional probability estimation with deep density models},
	Year = {2013}}

@incollection{mackay1997density,
	Author = {D. J. C.  MacKay and M. N. Gibbs},
	Booktitle = {Statistics and Neural Networks},
	Date-Added = {2018-05-14 16:01:27 +0000},
	Date-Modified = {2018-05-14 16:02:49 +0000},
	Pages = {129-146},
	Publisher = {Oxford},
	Rss-Description = {@Incollection{MacKay97:dn,
  author = 	 "D. J. C.  MacKay and M. N. Gibbs",
  title = 	 "Density Networks",
  publisher = "O.U.P.",
  booktitle={Statistics and Neural Networks},
  annote={subtitle: Advances at the Interface},
  annote={Proceedings of meeting on Statistics and Neural Nets, Edinburgh, 1997},
  year = 	 "1998",
  editor = 	 "J. W. Kay and D. M. Titterington",
  pages = 	 "129-146"
}},
	Title = {Density Networks},
	Year = {1997}}

@inproceedings{andrychowicz2016learning,
	Author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and de Freitas, Nando},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Added = {2018-05-14 15:49:45 +0000},
	Date-Modified = {2018-05-14 15:49:45 +0000},
	Pages = {3981--3989},
	Title = {Learning to learn by gradient descent by gradient descent},
	Year = {2016}}

@article{rezende2015variational,
	Author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	Date-Added = {2018-05-14 15:48:38 +0000},
	Date-Modified = {2018-05-14 15:48:38 +0000},
	Journal = {arXiv preprint arXiv:1505.05770},
	Title = {Variational inference with normalizing flows},
	Year = {2015}}

@inproceedings{titsias2014doubly,
	Author = {Titsias, Michalis and L{\'a}zaro-Gredilla, Miguel},
	Booktitle = {International Conference on Machine Learning},
	Date-Added = {2018-05-14 15:47:03 +0000},
	Date-Modified = {2018-05-14 15:47:03 +0000},
	Pages = {1971--1979},
	Title = {Doubly stochastic variational Bayes for non-conjugate inference},
	Year = {2014}}

@book{gelman2014bayesian,
	Author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
	Date-Added = {2018-05-14 15:45:05 +0000},
	Date-Modified = {2018-05-14 15:45:05 +0000},
	Publisher = {CRC press Boca Raton, FL},
	Title = {Bayesian data analysis},
	Volume = {2},
	Year = {2014}}

@article{mccullagh2002,
	Abstract = {This paper addresses two closely related questions, "What is a statistical model?" and "What is a parameter?" The notions that a model must "make sense," and that a parameter must "have a well-defined meaning" are deeply ingrained in applied statistical work, reasonably well understood at an instinctive level, but absent from most formal theories of modelling and inference. In this paper, these concepts are defined in algebraic terms, using morphisms, functors and natural transformations. It is argued that inference on the basis of a model is not possible unless the model admits a natural extension that includes the domain for which inference is required. For example, prediction requires that the domain include all future units, subjects or time points. Although it is usually not made explicit, every sensible statistical model admits such an extension. Examples are given to show why such an extension is necessary and why a formal theory is required. In the definition of a subparameter, it is shown that certain parameter functions are natural and others are not. Inference is meaningful only for natural parameters. This distinction has important consequences for the construction of prior distributions and also helps to resolve a controversy concerning the Box-Cox model.},
	Author = {Peter McCullagh},
	Date-Added = {2018-05-14 15:44:01 +0000},
	Date-Modified = {2018-05-14 15:44:16 +0000},
	Issn = {00905364},
	Journal = {The Annals of Statistics},
	Number = {5},
	Pages = {1225--1267},
	Publisher = {Institute of Mathematical Statistics},
	Title = {What Is a Statistical Model?},
	Url = {http://www.jstor.org/stable/1558705},
	Volume = {30},
	Year = {2002},
	Bdsk-Url-1 = {http://www.jstor.org/stable/1558705}}

@article{zhou2018compressibility,
	Author = {Zhou, Wenda and Veitch, Victor and Austern, Morgane and Adams, Ryan P and Orbanz, Peter},
	Date-Added = {2018-04-25 20:06:17 +0000},
	Date-Modified = {2018-04-25 20:06:17 +0000},
	Journal = {arXiv preprint arXiv:1804.05862},
	Title = {Compressibility and Generalization in Large-Scale Deep Learning},
	Year = {2018}}

@article{wainwright2008graphical,
	Author = {Wainwright, Martin J and Jordan, Michael I and others},
	Date-Added = {2018-04-25 19:54:15 +0000},
	Date-Modified = {2018-04-25 19:54:15 +0000},
	Journal = {Foundations and Trends{\textregistered} in Machine Learning},
	Number = {1--2},
	Pages = {1--305},
	Publisher = {Now Publishers, Inc.},
	Title = {Graphical models, exponential families, and variational inference},
	Volume = {1},
	Year = {2008}}

@book{Devroye:1986aa,
	Address = {New York},
	Annote = {LDR    00793pam  2200253 a 4500
001    4295721
005    20161013091938.0
008    860220s1986    nyu      b    001 0 eng  
906    $a7$bcbc$corignew$d1$eocip$f19$gy-gencatlg
925 0  $aacquire$b1 shelf copy$xpolicy default
010    $a   86003783 
020    $c$54.00 (est.)
035    $9(DLC)   86003783
040    $aDLC$cDLC$dDLC
050 00 $aQA274$b.D48 1986
082 00 $a519.2$219
100 1  $aDevroye, Luc.
245 10 $aNon-uniform random variate generation /$cLuc Devroye.
260    $aNew York :$bSpringer-Verlag,$cc1986.
300    $axvi, 843 p. ;$c25 cm.
504    $aBibliography: p. [784]-816.
500    $aIncludes index.
650  0 $aRandom variables.
991    $bc-GenColl$hQA274$i.D48 1986$p00035215815$tCopy 1$wBOOKS
},
	Author = {Devroye, Luc},
	Call-Number = {QA274},
	Date-Added = {2018-04-25 19:26:17 +0000},
	Date-Modified = {2018-04-25 19:26:17 +0000},
	Dewey-Call-Number = {519.2},
	Genre = {Random variables},
	Library-Id = {86003783},
	Publisher = {Springer-Verlag},
	Title = {Non-uniform random variate generation},
	Year = {1986}}

@article{rezende2014stochastic,
	Author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	Date-Added = {2018-04-25 19:11:29 +0000},
	Date-Modified = {2018-04-25 19:11:29 +0000},
	Journal = {arXiv preprint arXiv:1401.4082},
	Title = {Stochastic backpropagation and approximate inference in deep generative models},
	Year = {2014}}

@article{gutmann2014statistical,
	Author = {Gutmann, Michael U and Dutta, Ritabrata and Kaski, Samuel and Corander, Jukka},
	Date-Added = {2018-04-25 19:10:35 +0000},
	Date-Modified = {2018-04-25 19:10:35 +0000},
	Journal = {arXiv preprint arXiv:1407.4981},
	Title = {Statistical inference of intractable generative models via classification},
	Year = {2014}}

@incollection{Goodfellow:2014aa,
	Author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	Booktitle = {Advances in Neural Information Processing Systems 27},
	Date-Added = {2018-04-25 19:09:32 +0000},
	Date-Modified = {2018-04-25 19:09:45 +0000},
	Editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
	Pages = {2672--2680},
	Publisher = {Curran Associates, Inc.},
	Title = {Generative Adversarial Nets},
	Url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	Year = {2014},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}}

@article{Mohamed:2016aa,
	Abstract = {Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.
},
	Author = {Shakir Mohamed and Balaji Lakshminarayanan},
	Date-Added = {2018-04-25 18:56:51 +0000},
	Date-Modified = {2018-04-25 18:56:51 +0000},
	Eprint = {1610.03483},
	Month = {10},
	Title = {Learning in Implicit Generative Models},
	Url = {https://arxiv.org/pdf/1610.03483},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1610.03483},
	Bdsk-Url-2 = {https://arxiv.org/pdf/1610.03483}}

@article{Kingma:2013aa,
	Abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.
},
	Author = {Diederik P Kingma and Max Welling},
	Date-Added = {2018-04-25 18:45:58 +0000},
	Date-Modified = {2018-04-25 18:45:58 +0000},
	Eprint = {1312.6114},
	Month = {12},
	Title = {Auto-Encoding Variational Bayes},
	Url = {https://arxiv.org/pdf/1312.6114},
	Year = {2013},
	Bdsk-Url-1 = {https://arxiv.org/abs/1312.6114},
	Bdsk-Url-2 = {https://arxiv.org/pdf/1312.6114}}
