%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for John Cunningham at 2018-04-25 16:06:18 -0400 


%% Saved with string encoding Unicode (UTF-8) 



@article{zhou2018compressibility,
	Author = {Zhou, Wenda and Veitch, Victor and Austern, Morgane and Adams, Ryan P and Orbanz, Peter},
	Date-Added = {2018-04-25 20:06:17 +0000},
	Date-Modified = {2018-04-25 20:06:17 +0000},
	Journal = {arXiv preprint arXiv:1804.05862},
	Title = {Compressibility and Generalization in Large-Scale Deep Learning},
	Year = {2018}}

@article{wainwright2008graphical,
	Author = {Wainwright, Martin J and Jordan, Michael I and others},
	Date-Added = {2018-04-25 19:54:15 +0000},
	Date-Modified = {2018-04-25 19:54:15 +0000},
	Journal = {Foundations and Trends{\textregistered} in Machine Learning},
	Number = {1--2},
	Pages = {1--305},
	Publisher = {Now Publishers, Inc.},
	Title = {Graphical models, exponential families, and variational inference},
	Volume = {1},
	Year = {2008}}

@book{Devroye:1986aa,
	Address = {New York},
	Annote = {LDR    00793pam  2200253 a 4500
001    4295721
005    20161013091938.0
008    860220s1986    nyu      b    001 0 eng  
906    $a7$bcbc$corignew$d1$eocip$f19$gy-gencatlg
925 0  $aacquire$b1 shelf copy$xpolicy default
010    $a   86003783 
020    $c$54.00 (est.)
035    $9(DLC)   86003783
040    $aDLC$cDLC$dDLC
050 00 $aQA274$b.D48 1986
082 00 $a519.2$219
100 1  $aDevroye, Luc.
245 10 $aNon-uniform random variate generation /$cLuc Devroye.
260    $aNew York :$bSpringer-Verlag,$cc1986.
300    $axvi, 843 p. ;$c25 cm.
504    $aBibliography: p. [784]-816.
500    $aIncludes index.
650  0 $aRandom variables.
991    $bc-GenColl$hQA274$i.D48 1986$p00035215815$tCopy 1$wBOOKS
},
	Author = {Devroye, Luc},
	Call-Number = {QA274},
	Date-Added = {2018-04-25 19:26:17 +0000},
	Date-Modified = {2018-04-25 19:26:17 +0000},
	Dewey-Call-Number = {519.2},
	Genre = {Random variables},
	Library-Id = {86003783},
	Publisher = {Springer-Verlag},
	Title = {Non-uniform random variate generation},
	Year = {1986}}

@article{rezende2014stochastic,
	Author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	Date-Added = {2018-04-25 19:11:29 +0000},
	Date-Modified = {2018-04-25 19:11:29 +0000},
	Journal = {arXiv preprint arXiv:1401.4082},
	Title = {Stochastic backpropagation and approximate inference in deep generative models},
	Year = {2014}}

@article{gutmann2014statistical,
	Author = {Gutmann, Michael U and Dutta, Ritabrata and Kaski, Samuel and Corander, Jukka},
	Date-Added = {2018-04-25 19:10:35 +0000},
	Date-Modified = {2018-04-25 19:10:35 +0000},
	Journal = {arXiv preprint arXiv:1407.4981},
	Title = {Statistical inference of intractable generative models via classification},
	Year = {2014}}

@incollection{Goodfellow:2014aa,
	Author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	Booktitle = {Advances in Neural Information Processing Systems 27},
	Date-Added = {2018-04-25 19:09:32 +0000},
	Date-Modified = {2018-04-25 19:09:45 +0000},
	Editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
	Pages = {2672--2680},
	Publisher = {Curran Associates, Inc.},
	Title = {Generative Adversarial Nets},
	Url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	Year = {2014},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}}

@article{Mohamed:2016aa,
	Abstract = {Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.
},
	Author = {Shakir Mohamed and Balaji Lakshminarayanan},
	Date-Added = {2018-04-25 18:56:51 +0000},
	Date-Modified = {2018-04-25 18:56:51 +0000},
	Eprint = {1610.03483},
	Month = {10},
	Title = {Learning in Implicit Generative Models
},
	Url = {https://arxiv.org/pdf/1610.03483},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1610.03483},
	Bdsk-Url-2 = {https://arxiv.org/pdf/1610.03483}}

@article{Kingma:2013aa,
	Abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.
},
	Author = {Diederik P Kingma and Max Welling},
	Date-Added = {2018-04-25 18:45:58 +0000},
	Date-Modified = {2018-04-25 18:45:58 +0000},
	Eprint = {1312.6114},
	Month = {12},
	Title = {Auto-Encoding Variational Bayes
},
	Url = {https://arxiv.org/pdf/1312.6114},
	Year = {2013},
	Bdsk-Url-1 = {https://arxiv.org/abs/1312.6114},
	Bdsk-Url-2 = {https://arxiv.org/pdf/1312.6114}}
